
import streamlit as st
import os
import requests
import json

from core.log_reader import read_logs
from core.log_parser import parse_logs
from core.detector import detect_threats
from core.severity_engine import assign_severity

# --------------------------------------------------
# Page Config
# --------------------------------------------------
st.set_page_config(
    page_title="Security Monitoring Dashboard",
    layout="wide"
)

st.title("üîê Security Monitoring Dashboard")
st.write("Mini SIEM | Login Threat Detection with Local GenAI (Ollama)")

# --------------------------------------------------
# File Paths
# --------------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOG_FILE = os.path.join(BASE_DIR, "logs", "logs_login.txt")

# --------------------------------------------------
# Pipeline
# --------------------------------------------------
def run_pipeline():
    raw_logs = read_logs(LOG_FILE)
    parsed_logs = parse_logs(raw_logs)
    alerts = detect_threats(parsed_logs)
    alerts = assign_severity(alerts)
    return parsed_logs, alerts

parsed_logs, alerts = run_pipeline()

# --------------------------------------------------
# GenAI Prompt Builder
# --------------------------------------------------
def generate_ai_prompt(alerts):
    return f"""
You are a senior Security Operations Center (SOC) analyst.

You are analyzing authentication security alerts generated by a SIEM system.
Do NOT invent data. Only reason from the alerts provided.

Alerts:
{json.dumps(alerts, indent=2)}

Tasks:
1. Summarize the incidents clearly
2. Identify attack patterns (brute force, credential stuffing, suspicious IPs)
3. Assess overall risk level
4. Recommend mitigation steps

Write in clear, professional SOC analyst language.
"""

# --------------------------------------------------
# Ollama LLM Call
# --------------------------------------------------
def call_ollama(prompt, model="mistral"):
    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }

    try:
        response = requests.post(url, json=payload, timeout=120)
        response.raise_for_status()
        return response.json().get("response", "")
    except requests.exceptions.RequestException:
        return None

# --------------------------------------------------
# Metrics
# --------------------------------------------------
st.subheader("üìä Metrics")

col1, col2, col3 = st.columns(3)
col1.metric("Total Logs", len(parsed_logs))
col2.metric("Total Alerts", len(alerts))
col3.metric(
    "High Severity Alerts",
    sum(1 for a in alerts if a["severity"] == "HIGH")
)

# --------------------------------------------------
# Alerts Section
# --------------------------------------------------
st.subheader("üö® Alerts")

if alerts:
    for alert in alerts:
        if alert["severity"] == "HIGH":
            st.error(alert)
        elif alert["severity"] == "MEDIUM":
            st.warning(alert)
        else:
            st.info(alert)
else:
    st.success("No threats detected üéâ")

# --------------------------------------------------
# GenAI Analysis Section
# --------------------------------------------------
st.subheader("üß† AI Security Analyst Explanation")

if alerts:
    ai_prompt = generate_ai_prompt(alerts)

    with st.spinner("üß† AI Analyst is analyzing alerts..."):
        ai_response = call_ollama(ai_prompt)

    if ai_response:
        st.markdown("### üìå SOC Analysis")
        st.write(ai_response)
    else:
        st.error(
            "‚ùå Ollama is not running.\n\n"
            "Start it using:\n\n"
            "`ollama run mistral`"
        )

    with st.expander("üîç View GenAI Prompt"):
        st.code(ai_prompt)
else:
    st.info("No alerts available for AI analysis.")
